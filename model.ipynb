{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lovel\\miniconda3\\envs\\med-chatbot\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPIEmbeddings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and process the medical book (PDF)\n",
    "def load_pdf(data_path):\n",
    "    loader = DirectoryLoader(data_path, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Split the text into chunks\n",
    "def split_text_into_chunks(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks = text_splitter.split_documents(documents)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Download the pre-trained HuggingFace embedding model\n",
    "def download_embeddings_model():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create FAISS index from the embeddings\n",
    "def create_faiss_index(text_chunks, embeddings):\n",
    "    # Convert chunks into embeddings\n",
    "    chunk_texts = [chunk.page_content for chunk in text_chunks]\n",
    "    chunk_embeddings = embeddings.embed_documents(chunk_texts)\n",
    "\n",
    "    # Initialize FAISS index\n",
    "    dimension = 384  # Embedding dimension from MiniLM-L6-v2\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "    # Convert embeddings to a numpy array and add to the FAISS index\n",
    "    embedding_array = np.array(chunk_embeddings).astype('float32')\n",
    "    index.add(embedding_array)\n",
    "\n",
    "    # Return the index and text chunks for retrieval\n",
    "    return index, chunk_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Search the FAISS index for the most similar text chunk based on query\n",
    "def search_index(query, index, embeddings, chunk_texts):\n",
    "    # Embed the query\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "    # Search for the most similar text in the index\n",
    "    D, I = index.search(np.array([query_embedding], dtype='float32'), k=1)  # Find the top match\n",
    "\n",
    "    # Get the matching chunk\n",
    "    matching_chunk = chunk_texts[I[0][0]] if I[0].size > 0 else \"No relevant information found.\"\n",
    "    \n",
    "    return matching_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Display the response clearly and appropriately\n",
    "def display_response(matching_chunk):\n",
    "    # Ensure that the response is clear and concise\n",
    "    max_length = 1000  # Define a maximum length for response\n",
    "    if len(matching_chunk) > max_length:\n",
    "        # Truncate long responses\n",
    "        response = matching_chunk[:max_length] + \"...\"\n",
    "    else:\n",
    "        response = matching_chunk\n",
    "    \n",
    "    # Print the response\n",
    "    print(\"Here is the relevant response to your query:\\n\")\n",
    "    print(response)\n",
    "    print(\"\\nIf you need more details, feel free to ask another question!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lovel\\miniconda3\\envs\\med-chatbot\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\lovel\\miniconda3\\envs\\med-chatbot\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lovel\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\lovel\\miniconda3\\envs\\med-chatbot\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the relevant response to your query:\n",
      "\n",
      "consistently identifiable cause, but researchers havesuggested that migraines or transient ischemic attacksmay be the trigger. (A transient ischemic attack ,\n",
      "\n",
      "If you need more details, feel free to ask another question!\n",
      "Exiting the chatbot. Have a great day!\n"
     ]
    }
   ],
   "source": [
    "# Main function to run the chatbot\n",
    "def run_chatbot():\n",
    "    data_path = \"data/\"\n",
    "    \n",
    "    # 1. Load the PDF\n",
    "    documents = load_pdf(data_path)\n",
    "\n",
    "    # 2. Split the text into chunks\n",
    "    text_chunks = split_text_into_chunks(documents)\n",
    "\n",
    "    # 3. Download the HuggingFace embedding model\n",
    "    embeddings = download_embeddings_model()\n",
    "\n",
    "    # 4. Create FAISS index\n",
    "    index, chunk_texts = create_faiss_index(text_chunks, embeddings)\n",
    "\n",
    "    # 5. Simulate a chat session\n",
    "    while True:\n",
    "        query = input(\"Ask a medical question (or type 'exit' to quit): \")\n",
    "        if query.lower() == 'exit':\n",
    "            print(\"Exiting the chatbot. Have a great day!\")\n",
    "            break\n",
    "        \n",
    "        matching_chunk = search_index(query, index, embeddings, chunk_texts)\n",
    "        \n",
    "        # Display the result in a clear and concise manner\n",
    "        display_response(matching_chunk)\n",
    "    \n",
    "# Run the chatbot\n",
    "run_chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
